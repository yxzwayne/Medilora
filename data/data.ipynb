{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC-III Dataset\n",
    "\n",
    "`patient_data.csv` is the cleaned patient data by teammate Ananay and Shreyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "df = pd.read_csv(\"patient_data.csv\")\n",
    "n = len(df.index)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the `notes` column of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_str(input):\n",
    "#     input = input.split()\n",
    "#     input = \" \".join(input).strip()\n",
    "#     return input\n",
    "\n",
    "# df[\"notes\"] = df[\"notes\"].apply(clean_str)\n",
    "\n",
    "# # Assuming df is your DataFrame\n",
    "# with open(f\"anonymized_patient_notes.txt\", 'w', encoding='utf-8') as file:\n",
    "#     for _, row in df.iterrows():\n",
    "#         file.write(row['notes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIMIC-III QA Pair Construction\n",
    "\n",
    "Attempt at constructing question-answer pairs based on the medical notes using local models, unsuccessful given the below code.\n",
    "\n",
    "For a higher chance of success, consider applying chatml format (construct a conversation list) first.\n",
    "\n",
    "Also, ollama is fairly slow. Consider using `llama-cpp-python`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "\n",
    "def ollama_request(prompt, model=\"hermes\"):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    param = {\"model\": \"hermes\", \"prompt\": prompt, \"stream\": False, \"raw\": True}\n",
    "    res = requests.post(url, json=param).json()\n",
    "    pprint(res)\n",
    "    bot_response = res[\"response\"]\n",
    "    sec = res[\"eval_duration\"] / 1000000000\n",
    "    tok_s = res[\"eval_count\"] / sec\n",
    "    return bot_response\n",
    "\n",
    "\n",
    "def obtain_qa(note, model=\"hermes\"):\n",
    "    q_prompt = f'MEDICAL NOTE: \"\"\"\\n{note}\"\"\" \\nBased on the given medical note, what is be the single most probably inquiry or question the patient asked to the doctor? '\n",
    "    q = ollama_request(q_prompt, model)\n",
    "\n",
    "    ans_prompt = f'PATIENT QUESTION:  \"\"\"\\n{q}\"\"\" \\n MEDICAL NOTE: \"\"\"\\n{note}\"\"\" \\nBased on the given medical note and patient question, construct a concise and terse paragraph of a top professional doctors response in 3 to 4 sentences. '\n",
    "    ans = ollama_request(ans_prompt)\n",
    "\n",
    "    return q, ans\n",
    "\n",
    "\n",
    "def obtain_qa_single_run(note, model=\"hermes\"):\n",
    "    prompt = f'MEDICAL NOTE: \"\"\"\\n{note}\"\"\" \\nBased on the given medical note, construct one `Question` and `Answer` pair between the patient and the doctor in JSON format with exactly one pair of `Question` and `Answer`. The patients question includes clear and detailed description of the problem relevant in the medical note. The doctors answer is in first person perspective, and includes reasoning and details, such as sympotoms, diagnosis, inference, suggestions, medications. Both questions and answers should be concise, straight to the point and highly medically relevant. '\n",
    "\n",
    "    res = ollama_request(prompt, model)\n",
    "    return res\n",
    "\n",
    "\n",
    "r = random.randint(0, n)\n",
    "\n",
    "sample_note = df.iloc[r].notes\n",
    "\n",
    "# q, ans = obtain_qa(sample_note)\n",
    "# print(f\"==== question: {q}\")\n",
    "# print(f\"==== answer: {ans}\")\n",
    "\n",
    "print(f\"\\n\\n {obtain_qa_single_run(sample_note, 'neural')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMedQA Conversation Construction\n",
    "\n",
    "Contains the transformation codes for both chatml and sharegpt format.\n",
    "\n",
    "Importantly, using `sharegpt` format chat datasets needs us to modify the system prompts in the axolotl source code file `src/axolotl/prompt_strategies/sharegpt.py` before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"teknium/OpenHermes-2.5-Mistral-7B\")\n",
    "\n",
    "dataset = load_dataset(\"pubmed_qa\", \"pqa_artificial\")[\"train\"]\n",
    "dataset = dataset.flatten().remove_columns(\n",
    "    [\"pubid\", \"context.labels\", \"context.meshes\"]\n",
    ")\n",
    "\n",
    "\n",
    "dataset = dataset.rename_column(\n",
    "    original_column_name=\"context.contexts\", new_column_name=\"contexts\"\n",
    ")\n",
    "\n",
    "# Axolotl configurations:\n",
    "# === alpaca_w_system.load_open_orca_chatml: \n",
    "# {\"system_prompt\": \"...\", \"question\": \"...\", \"response\": \"...\"} - \n",
    "# === sharegpt: \n",
    "# {\"conversations\": [{\"from\": \"...\", \"value\": \"...\"}]} - sharegpt. \n",
    "# Using sharegpt format demands manually modify system prompt at:\n",
    "# src/axolotl/prompt_strategies/sharegpt.py\n",
    "\n",
    "def concat_contexts(row):\n",
    "    system = \"As an expert doctor in clinical science and medical knowledge, can you tell me if the following question is correct, given the accompanying context? Answer yes, no, or maybe. Then, follow up with some explanations.\"\n",
    "    user = \"Context: \" + \" \".join(row[\"contexts\"]) + \" Question: \" + row[\"question\"]\n",
    "    assistant = row[\"final_decision\"] + \". \" + row[\"long_answer\"]\n",
    "    chat = [\n",
    "        # {\"from\": \"system\", \"value\": system},\n",
    "        {\"from\": \"user\", \"value\": user},\n",
    "        {\"from\": \"assistant\", \"value\": assistant},\n",
    "    ]\n",
    "    row[\"conversations\"] = chat\n",
    "    return row\n",
    "\n",
    "\n",
    "dataset = dataset.map(concat_contexts, num_proc=12)\n",
    "# dataset = dataset.add_column(name=\"context\", column=contexts)\n",
    "dataset = dataset.remove_columns(\n",
    "    [\"contexts\", \"question\", \"final_decision\", \"long_answer\"]\n",
    ")\n",
    "\n",
    "# dataset.push_to_hub(\"Medilora/PubMedQA-ShareGPT\", private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check chat template format and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"teknium/OpenHermes-2.5-Mistral-7B\")\n",
    "\n",
    "sample_chat = dataset[\"chat\"][2]\n",
    "\n",
    "tokenized_chat = tokenizer.apply_chat_template(sample_chat, tokenize=True, return_tensors=\"pt\")\n",
    "print(tokenizer.decode(tokenized_chat[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
