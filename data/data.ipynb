{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC-III Dataset\n",
    "\n",
    "`patient_data.csv` is the cleaned patient data by teammate Ananay and Shreyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "df = pd.read_csv(\"patient_data.csv\")\n",
    "n = len(df.index)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the `notes` column of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_str(input):\n",
    "#     input = input.split()\n",
    "#     input = \" \".join(input).strip()\n",
    "#     return input\n",
    "\n",
    "# df[\"notes\"] = df[\"notes\"].apply(clean_str)\n",
    "\n",
    "# # Assuming df is your DataFrame\n",
    "# with open(f\"anonymized_patient_notes.txt\", 'w', encoding='utf-8') as file:\n",
    "#     for _, row in df.iterrows():\n",
    "#         file.write(row['notes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIMIC-III QA Pair Construction\n",
    "\n",
    "Attempt at constructing question-answer pairs based on the medical notes using local models, unsuccessful given the below code.\n",
    "\n",
    "For a higher chance of success, consider applying chatml format (construct a conversation list) first.\n",
    "\n",
    "Also, ollama is fairly slow. Consider using `llama-cpp-python`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "\n",
    "def ollama_request(prompt, model=\"hermes\"):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    param = {\"model\": \"hermes\", \"prompt\": prompt, \"stream\": False, \"raw\": True}\n",
    "    res = requests.post(url, json=param).json()\n",
    "    pprint(res)\n",
    "    bot_response = res[\"response\"]\n",
    "    sec = res[\"eval_duration\"] / 1000000000\n",
    "    tok_s = res[\"eval_count\"] / sec\n",
    "    return bot_response\n",
    "\n",
    "\n",
    "def obtain_qa(note, model=\"hermes\"):\n",
    "    q_prompt = f'MEDICAL NOTE: \"\"\"\\n{note}\"\"\" \\nBased on the given medical note, what is be the single most probably inquiry or question the patient asked to the doctor? '\n",
    "    q = ollama_request(q_prompt, model)\n",
    "\n",
    "    ans_prompt = f'PATIENT QUESTION:  \"\"\"\\n{q}\"\"\" \\n MEDICAL NOTE: \"\"\"\\n{note}\"\"\" \\nBased on the given medical note and patient question, construct a concise and terse paragraph of a top professional doctors response in 3 to 4 sentences. '\n",
    "    ans = ollama_request(ans_prompt)\n",
    "\n",
    "    return q, ans\n",
    "\n",
    "\n",
    "def obtain_qa_single_run(note, model=\"hermes\"):\n",
    "    prompt = f'MEDICAL NOTE: \"\"\"\\n{note}\"\"\" \\nBased on the given medical note, construct one `Question` and `Answer` pair between the patient and the doctor in JSON format with exactly one pair of `Question` and `Answer`. The patients question includes clear and detailed description of the problem relevant in the medical note. The doctors answer is in first person perspective, and includes reasoning and details, such as sympotoms, diagnosis, inference, suggestions, medications. Both questions and answers should be concise, straight to the point and highly medically relevant. '\n",
    "\n",
    "    res = ollama_request(prompt, model)\n",
    "    return res\n",
    "\n",
    "\n",
    "r = random.randint(0, n)\n",
    "\n",
    "sample_note = df.iloc[r].notes\n",
    "\n",
    "# q, ans = obtain_qa(sample_note)\n",
    "# print(f\"==== question: {q}\")\n",
    "# print(f\"==== answer: {ans}\")\n",
    "\n",
    "print(f\"\\n\\n {obtain_qa_single_run(sample_note, 'neural')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMedQA Conversation Construction\n",
    "\n",
    "Contains the transformation codes for both chatml and sharegpt format.\n",
    "\n",
    "Importantly, using `sharegpt` format chat datasets needs us to modify the system prompts in the axolotl source code file `src/axolotl/prompt_strategies/sharegpt.py` before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"teknium/OpenHermes-2.5-Mistral-7B\")\n",
    "\n",
    "dataset = load_dataset(\"pubmed_qa\", \"pqa_artificial\")[\"train\"]\n",
    "dataset = dataset.flatten().remove_columns(\n",
    "    [\"pubid\", \"context.labels\", \"context.meshes\"]\n",
    ")\n",
    "\n",
    "\n",
    "dataset = dataset.rename_column(\n",
    "    original_column_name=\"context.contexts\", new_column_name=\"contexts\"\n",
    ")\n",
    "\n",
    "# Axolotl configurations:\n",
    "# === alpaca_w_system.load_open_orca_chatml: \n",
    "# {\"system_prompt\": \"...\", \"question\": \"...\", \"response\": \"...\"} - \n",
    "# === sharegpt: \n",
    "# {\"conversations\": [{\"from\": \"...\", \"value\": \"...\"}]} - sharegpt. \n",
    "# Using sharegpt format demands manually modify system prompt at:\n",
    "# src/axolotl/prompt_strategies/sharegpt.py\n",
    "\n",
    "def concat_contexts(row):\n",
    "    system = \"As an expert doctor in clinical science and medical knowledge, can you tell me if the following question is correct, given the accompanying context? Answer yes, no, or maybe. Then, follow up with some explanations.\"\n",
    "    user = \"Context: \" + \" \".join(row[\"contexts\"]) + \" Question: \" + row[\"question\"]\n",
    "    assistant = row[\"final_decision\"] + \". \" + row[\"long_answer\"]\n",
    "    chat = [\n",
    "        # {\"from\": \"system\", \"value\": system},\n",
    "        {\"from\": \"user\", \"value\": user},\n",
    "        {\"from\": \"assistant\", \"value\": assistant},\n",
    "    ]\n",
    "    row[\"conversations\"] = chat\n",
    "    return row\n",
    "\n",
    "\n",
    "dataset = dataset.map(concat_contexts, num_proc=12)\n",
    "# dataset = dataset.add_column(name=\"context\", column=contexts)\n",
    "dataset = dataset.remove_columns(\n",
    "    [\"contexts\", \"question\", \"final_decision\", \"long_answer\"]\n",
    ")\n",
    "\n",
    "# dataset.push_to_hub(\"Medilora/PubMedQA-ShareGPT\", private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check chat template format and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'chat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/yuxuan/medilora/medilora/data/data.ipynb Cell 10\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yuxuan/medilora/medilora/data/data.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39m\u001b[39mMedilora/PubMedQA-ShareGPT\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yuxuan/medilora/medilora/data/data.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mteknium/OpenHermes-2.5-Mistral-7B\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yuxuan/medilora/medilora/data/data.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m sample_chat \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39;49m\u001b[39mchat\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m2\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yuxuan/medilora/medilora/data/data.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m tokenized_chat \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mapply_chat_template(sample_chat, tokenize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yuxuan/medilora/medilora/data/data.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(tokenizer\u001b[39m.\u001b[39mdecode(tokenized_chat[\u001b[39m0\u001b[39m]))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/datasets/dataset_dict.py:59\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, k) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dataset:\n\u001b[1;32m     58\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(k, (\u001b[39mstr\u001b[39m, NamedSplit)) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 59\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(k)\n\u001b[1;32m     60\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m         available_suggested_splits \u001b[39m=\u001b[39m [\n\u001b[1;32m     62\u001b[0m             split \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m (Split\u001b[39m.\u001b[39mTRAIN, Split\u001b[39m.\u001b[39mTEST, Split\u001b[39m.\u001b[39mVALIDATION) \u001b[39mif\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m     63\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'chat'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"teknium/OpenHermes-2.5-Mistral-7B\")\n",
    "\n",
    "sample_chat = dataset[\"chat\"][2]\n",
    "\n",
    "tokenized_chat = tokenizer.apply_chat_template(sample_chat, tokenize=True, return_tensors=\"pt\")\n",
    "print(tokenizer.decode(tokenized_chat[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Datasets Token Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 523/523 [00:00<00:00, 4.93MB/s]\n",
      "Downloading data: 100%|██████████| 5.31M/5.31M [00:00<00:00, 6.86MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 903.36it/s]\n",
      "Generating train split: 100%|██████████| 10178/10178 [00:00<00:00, 318519.87 examples/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Tokenizing texts: 100%|██████████| 10178/10178 [00:05<00:00, 1874.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens = 3617857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset_id = \"Medilora/MedMCQA-ShareGPT\"\n",
    "model_id = \"Medilora/guideline-medilora-adapter\"\n",
    "\n",
    "dataset = load_dataset(dataset_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "texts = dataset[\"train\"][\"conversations\"]\n",
    "n_tokens = 0\n",
    "\n",
    "for entry in tqdm(texts, desc=\"Tokenizing texts\"):\n",
    "    for chat in entry:\n",
    "        tokens = tokenizer.tokenize(chat[\"value\"])\n",
    "        n_tokens += len(tokens)\n",
    "\n",
    "print(f\"Total tokens = {n_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'from': 'user', 'value': 'Context: Psammaplin A (PsA) is a natural product isolated from marine sponges, which has been demonstrated to have anticancer activity against several human cancer cell lines via the induction of cell cycle arrest and apoptosis. New drugs that are less toxic and more effective against multidrug-resistant cancers are urgently needed. We tested cell proliferation, cell cycle progression and autophagic cell death pathway in doxorubicin-resistant MCF-7 (MCF-7/adr) human breast cancer cells. The potency of PsA was further determined using an in vivo xenograft model. Question: Does psammaplin A induce Sirtuin 1-dependent autophagic cell death in doxorubicin-resistant MCF-7/adr human breast cancer cells and xenografts?'}\n",
      "{'from': 'assistant', 'value': 'yes. PsA significantly inhibited MCF-7/adr cells proliferation in a concentration-dependent manner, with accumulation of cells in G2/M phase of the cell cycle. PsA significantly decreased SIRT1 enzyme activity and reduced expression of SIRT1 protein in the cultured cells with greater potency than sirtinol or salermide. Acetylation of p53, a putative target of SIRT1, increased significantly following PsA treatment. In addition, PsA markedly increased the expression levels of autophagy-related proteins. In support of this, it was found that PsA significantly increased the expression of damage-regulated autophagy modulator (DRAM), a p53-induced protein.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"Medilora/PubMedQA-ShareGPT\")\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "sampledata = data[\"train\"][\"conversation\"][2]\n",
    "\n",
    "for k in sampledata:\n",
    "    print(k)\n",
    "    # print(f\"{k}: {sampledata[k]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
