# MediLora

Finetuning LLMs on medical text data to elicit differential diagnosis.

## Steps to take on a new instance

- `pip install -r requirements` (for logging the runs on wandb)
- `wandb login`
- `huggingface-cli login` (Connect HuggingFace for Data and Model sharing)
- If using notebook, `pip install huggingface_hub`, then run the following in a cell:

```
from huggingface_hub import notebook_login
notebook_login()
```

## ToDo
-[] fix sft script on multi-gpu server

## Running continued training scripts

`python3 train_continued_nous.py`

- The script uses the base `trainer` from ðŸ¤— Transformers and do not need accelerate.

## Running Supervised Finetuning scripts

```
accelerate config # will prompt you to define the training configuration
accelerate launch train_sft_nous.py # launches training
```

## Related Work

### [ChatDoctor](https://arxiv.org/pdf/2303.14070.pdf)

Their training set up for healthCareMagic-100k was 3 hours on 6 A100 GPUs.

Training parameters:

- batch size 192
- learning rate 2e-5
- epoch 3
- max seq length 512
- warmup 0.03
- weight decay none

They constructed databases using MedlinePlus and Wikipedia as external knowledge base for inference retrieval. The construction framework of the knowledge base was claimed to be extendable to reliable online databases, such as reputable academic journals.

Format of External Database:

Disease: ...
Symptoms: ...
Further Test: ...
Treatment: ...

Their evaluation was using contemporary medical queries containing recent medical news.

### [BioGPT](https://academic.oup.com/bib/article/23/6/bbac409/6713511)

Classical literature. Uses GPT-2 architecture on various medical NLP tasks, not limited to QA.

## Datasets

### HealthCareMagic

The dataset used in the ChatDoctor paper. We use [lavita/ChatDoctor-HealthCareMagic-100k](https://huggingface.co/datasets/xDAN-datasets/ChatDoctor_HealthCareMagic_112k) from HF - Columns: `instruction`, `input`, `output` - Its instruction is hilarious and we should use our own.

### PubMedQA

Medical QA dataset. There are various format options we can consider.

- [Base Official Dataset](https://huggingface.co/datasets/pubmed_qa)
  - 211k rows
  - Columns: `pubid`, `question`, `context` (sequence), `long_answer`, `final_decision`
- [BigBio Dataset](https://huggingface.co/datasets/bigbio/pubmed_qa)
  - 211k rows with 200k in train,
  - Columns: `id`, `question`, `type` (yesno), `choices`, `context`, `answer` (yes or no)
  - Trivially nicer to work with because the columns are already flattened.
- [FedML Dataset](https://huggingface.co/datasets/FedML/PubMedQA_instruction)
  - 273k rows, 1k test ???
  - This is an instruction tuning datase.
  - Columns: `instruction`, `context`, `response`.

The FedML dataset is probably

Some thoughts on using pubmed dataset:

- Wayne thought about adding the answer (yes or no) to the context, but the question of whether adding it to the start or end is a tricky one. Adding it to the end risks delaying immediate response that the users may be interested in, thus risking lower helpfulness; on the other hand, it is not obvious what the effect of adding yes/no to the start of context is on performance.
  Related to this consideration are concerns that the first token may largely determine the output distribution. It was shown that the first and first few tokens being decoded (the model first says) can act as a way to breach safety RLHF.

[PubMedQA repo](https://pubmedqa.github.io/) offers evaluation scripts. Will check repo and run benchmark against top 5 models.

### US Medical License Exam Textbooks

Raw texts of the 18 textbooks as continued training. Uploaded by Wayne for project use only.
`yxzwayne/USMedicalLicenseExamsTextbooks`

## Model Checkpoints Considered

- From Nous Research:

  - teknium/OpenHermes-2.5-Mistral-7B
  - NousResearch/Nous-Hermes-llama-2-7b
  - NousResearch/Nous-Hermes-Llama2-13b

- From Microsoft:
  - microsoft/Orca-2-7b
  - microsoft/Orca-2-13b

Notes

1. Mistral-7b on OpenHermes benched higher than on llama2-13b, which makes sense because mistral is a llama2 finetune.
2. Orca-2-13b underperforms OpenHermes-2.5-Mistral-7B on BigBench, which is quite a reflection on its reasoning abilities.

## Repo Referenced

- https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama_2/scripts/README.md

## Reading Notes

From llama2 paper:

> We found that SFT annotations in the order of tens of thousands was enough to achieve a high-quality result.

> To validate our data quality, we carefully examined a set of 180 examples, comparing the annota- tions provided by humans with the samples generated by the model through manual scrutiny.

> For supervised fine-tuning, we use a cosine learning rate schedule with an initial learning rate of 2eâˆ’5, a weight decay of 0.1, a batch size of 64, and a sequence length of 4096 tokens.

> We utilize an autoregressive objective and zero-out the loss on tokens from the user prompt, so as a result, we backpropagate only on answer tokens.
